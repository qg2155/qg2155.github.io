---
title: "Sample Assignment"
output:
  html_document: 
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, include = FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)
library(hexbin)
```

```{r page setup, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 1,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Description
This is a sample assignment that I completed for my Data Science course.  
In this assignment, we were given three different datasets to analyze.  
* Part 1: BRFSS Data    
Analyze overall health ratings across states from 2002 to 2010.  
* Part 2: Instacart Data  
Analyze ordered items and ordering trends of people's purchases from Instacart.  
* Part 3: NY NOAA Data  
Analyze temperature and precipitation trends in New York.

## Part1

### Clean dataset 

```{r get brfss, include = FALSE}
data("brfss_smart2010")
```

```{r clean brfss}
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = str_c(c("Excellent" , "Very good", "Good", "Fair", "Poor"))))
```

### Summarize distinct locations 

```{r states}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc))
```

In 2002, there were 3 states that were observed at 7 locations. These 3 states were CT, FL, and NC. 

### Create spaghetti plot of number locations 

```{r spaghetti plot}
brfss_data %>% 
  group_by(locationabbr, year) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>% 
  ggplot(aes(x = year, y = n_locations, color = locationabbr)) + 
    geom_point() + geom_line()
```

This spaghetti plot shows the number of locations in each state from 2002 to 2010. Florida in particular seems to have over 40 distinct locations in two different time points. Majority of states seem to have around 0 to 10 different locations across the years. 

### Table for mean and standard deviation of "Excellent" responses in NY

```{r  excellent responses}
brfss_data %>%
  group_by(year) %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  filter(locationabbr == "NY") %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)
```

Witin the state of New York, 2002 has the highest average of 24% for excellent responses, and a standard deviation of 4.5. Year 2006 has an average of 22.5% and standard deviation of 4.0. Year 2010 has an average of 22.7% and standard deviation of 3.6. 

### Plot for distribution of each response category across states

```{r five plots}
brfss_data %>% 
  group_by(year, locationabbr, response) %>%
  summarize(mean_response = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_response)) +
    geom_violin() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    facet_grid(. ~ response) 
```

Across all states and years, the response category of "Very good" seems to have the highest average, followed by "Good", "Excellent", "Fair", and "Poor". Average excellent responses range from 12 to almost 30%. Very good ranges from 16 to 43%. Good ranges from 24 to 36%. Fair ranges from 5 to 17%. Poor ranges from 2 to 9%. Very good responses seem to have the most variance whereas poor responses seem to have the least variance.  

## Part 2

### Clean dataset

```{r get instacart, include = FALSE}
data("instacart")
```

```{r clean instacart}
instacart_data = instacart %>% 
  janitor::clean_names() 
```

This instacart dataset has `r ncol(instacart_data)` variables and `r nrow(instacart_data)` observations. The variables are order_id, product_id, add_to_cart_order, reordered, user_id, eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order, product_name, aisle_id, department_id, aisle, and department. Out of all these variables, the key ones include order_id, product_id, reordered, order_dow, order_hour_of_day, days_since_prior_order, product_name, aisle, and department. These variables can give us a lot of information on popularity of different products, and customer behaviors (day and time or orderes, days in between orders). 

Here is a demonstration of reading observation for customer with user_id 1:
This customer's order_id is 1187899, and they ordered 11 items in this order. The products they ordered are added to cart in this order: soda, organic string cheese, 0% greek strained yogurt, XL paper towel rolls, milk chocolate almonds, postachios, cinnamon toast crunch, aged white cheddar popcorn, organic whole milk, organic half & half, and zero calorie cola. Most of these items belong in the snacks and dairy eggs departments. This ordered was placed on a Thursday and 8 AM. It has been 14 days since their prior order. 

### Aisles and most items ordered 

```{r most items}
instacart_data %>% 
  group_by(aisle_id) %>% 
  summarize(n_most_ordered = n()) %>% 
  arrange(desc(n_most_ordered)) 
```

There are `r instacart_data %>% summarize(n_aisles = n_distinct(aisle_id))` aisles. Aisle 83 has the most item ordered from because 150609 items were ordered from that aisle. This is followed by aisle 24 with 150473 items, and aisle 123 with 78493 items. 

### Plot for number of items ordered in each aisle

```{r aisle plot}
instacart_data %>% 
  group_by(aisle_id) %>% 
  summarize(n_most_ordered = n()) %>%
  arrange(desc(n_most_ordered)) %>% 
  mutate(aisle_id = as.factor(aisle_id)) %>% 
  mutate(aisle_id = forcats::fct_reorder(aisle_id, n_most_ordered)) %>% 
  ggplot(aes(x = aisle_id, y = n_most_ordered)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Items ordered from aisle",
      x = "Aisle ID",
      y = "Count of items"
    ) + 
  scale_x_discrete(breaks = c("132", "68", "39", "60", "1", "89", "23", "4", "108", "78" , "83"))
```

This bar graph shows that two aisles in particular have a much greater items ordered compared to the rest of the aisles. On the other end, there are also a good number of aisles that have barely any items ordered. A majority of the aisles get anywhere around 0 to 25000 items ordred. 

### Table for most popular item in each of the three aisles

```{r popular item}
instacart_data %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  summarize(n_ordered = n()) %>% 
  arrange(desc(n_ordered)) %>%
  top_n(1) %>% 
  knitr::kable()
```

The most popular product in the "packaged vegetables fruits" aisle is Organic Baby Spinach. It was ordered 9784 times. The most popular product in the "baking ingredients" aisle is Light Brown Sugar. It was ordered 499 times. The most popular product in the "dog food care" aisle is Snack Sticks Chicken & Rice Recipe Dog Treats. It was ordered 30 times. 

### Table for Pink Lady Apples and Coffee Ice Cream Order Times

```{r}
instacart_data %>% 
  select(order_id, product_id, order_dow, order_hour_of_day, product_name, aisle_id) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_order_hour) %>% 
  knitr::kable(digits = 2)
```

The average order time for Coffee Ice Cream ranges from 12:26 to 15:38 throughout the week. The day with the earliest average order time is Friday and the day with the latest average order time is Tuesday. Pink Lady Apples have an overall earlier average order time than Coffee Ice Cream. The range for Pink Lady Apples average order times are 11:36 to 14:25. The day with the earliest average order time is Monday and the day with the latest average order time is Wednesday. 

## Part 3

### Clean dataset

```{r get noaa, include = FALSE}
data("ny_noaa")
```

```{r clean noaa}
noaa_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "date"), sep = "-") %>% 
  mutate(prcp = prcp/10,
         tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10
         ) 
```
Cleaned variable names and separated date into year, month, and date columns. Transformed the precipitation values from tenths of mm to mm by dividing original values by 10. Same is done to transform the minimum and maximum temperatures from tenths of Celsius to Celsius. 

### To find the proportion of missing values
```{r missing values}
noaa_data %>% 
  summarize(n_missing_prcp = mean(is.na(prcp)),
            n_missing_snow = mean(is.na(snow)),
            n_missing_snwd = mean(is.na(snwd)),
            n_missing_tmax = mean(is.na(tmax)),
            n_missing_tmin = mean(is.na(tmin))
            )
```

This NY NOAA dataset has `r ncol(noaa_data)` variables and `r nrow(noaa_data)` observations. The original 7 variables are id, date, prcp, snow, snwd, tmax, and tmin. After separting date into year, month, and day, there are now an additional 2 variables to make a total of 9 variables in the cleaned dataset. The key variables are date, precipitation, snow, snow depth, maximum and minimum temperatures. These variables will be able to give us a lot of information on the weather pattern in New York, especially for precipitation and snow in the winters. 

One problem with this dataset is that there are many missing values for snow, snwd, tmax and tmin. This is probably due to the fact that some stations only collect a subset of variables. The proportion of missing in precipitation column is 5.62%, snow is 14.7%, snow depth is 22.8%, tmax and tmin are both 43.7%. The extent in which data is missing is problematic for our analysis. It will be difficult to get accurate weather patterns across NY since we might be missing critical pieces of information from stations that may be situated in locations that are in particular warmer or colder than average. 

```{r snowfall}
noaa_data %>% 
  group_by(snow) %>% 
  summarize(n_snow = n()) %>% 
  arrange(desc(n_snow))
```

The most commonly observed value for snowfall is 0 mm, with 2008508 observations. This is the expected value because it does not snow on most days throughout the years. The second commonly observed value is NA because this information is missing for many observations. The third commonly observed value is 25 mm with 31022 observations. The fourth commonly observed value is 13 mm with 23095 observations. These values tell us that when it does snow in NY, it is usually very light. 

### Plot of average max temp in Jan and July 

```{r avg max}
noaa_data %>% 
  filter(month %in% c("01", "07")) %>% 
  group_by(year, month, date) %>%
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_mean)) +
    geom_boxplot() +
    facet_grid(. ~month) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    labs(
      title = "Average max temperature in Janurary and July across years",
      x = "Years",
      y = "Average max temperature (C)"
    )
```

The graphs show that there is an appreciable difference between the average maximum temperatures between Jan and July across all years. The range for the average max temp in Jan is anywhere from -15 C to 15 C whereas the average for July is between 20 C to 34 C. There are outliers in some of the years for both months. July has more outliers for low temperatures across years whereas Jan has a relatively equal number of outliers for warmer and cooler average maximum temperatures. Year 1988 had an outlier for especially low temperature (17C) in July. The average maximum temperatures in July also have a smaller variance compared to Janurary judging by the size of the box plots. 

### Two panel plot for temperatures and snowfall

```{r two panel plot}
tmin_tmax_plot = ggplot(noaa_data, aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Temperature Plot",
    x = "Minimum daily temperature (C)",
    y = "Maximum daily temperature (C)"
  )
snowfall_plot = noaa_data %>% 
  filter(snow > 0 & snow < 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Snowfall(0>snow<100) by Year",
    x = "Year",
    y = "Distribution of Snow"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
tmin_tmax_plot/snowfall_plot
```

The first tmax and tmin plot shows that the minimum daily temperatures can range from -45 C to 40 C, with outliers near -60 C on the cold end and up to 60 C on the warm end. On the other end, maximum temperatures can range from -30 C to 40 C, with outliers as cold as -40 C and as hot as 60 C. The light shade of blue in the center of the cluster represents the range for the most common maximum and minimum temperatures. This means the most observed minimum temperature in NY is from 0 C to 15 C, and the most observed maximum temperature is from 10 C to 30 C. 

The second box plot shows the distribution of snowfall across all years. This box plot shows a rather stable distribution of snowfall across most years because the boxes are very similar to one another. The average distribution of snowfall is around 25 mm across all years. Years 1998, 2004, 2006, 2007, and 2010 had noticibly less snowfalls than usual. However, year 2006 also had many outliers for high snall fall ranging up to 100 mm. Years 1998 and 2010 also had a few outliers for heavy snowfall. 